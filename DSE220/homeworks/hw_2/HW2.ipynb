{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Data\n",
    "Download the MNIST train and test data from github along with their corre- sponding label files. The train and test data consist of 6000 and 1000 binarized MNIST images respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"mnist_train_data.csv\", header=None)\n",
    "train_label = pd.read_csv(\"mnist_train_labels.csv\", header=None, names=['label'])\n",
    "test_data = pd.read_csv(\"mnist_test_data.csv\", header=None)\n",
    "test_label = pd.read_csv(\"mnist_test_labels.csv\", header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Generative Learning\n",
    "Please don’t use the direct function from scikit-learn library for questions 1, 2, 3 and write your own implementation for them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1: Compute and report the prior probabilities πj for all labels. (10 marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior Probability of label: 0  =  0.09866666666666667\n",
      "Prior Probability of label: 1  =  0.11183333333333334\n",
      "Prior Probability of label: 2  =  0.09683333333333333\n",
      "Prior Probability of label: 3  =  0.10133333333333333\n",
      "Prior Probability of label: 4  =  0.10383333333333333\n",
      "Prior Probability of label: 5  =  0.08566666666666667\n",
      "Prior Probability of label: 6  =  0.10133333333333333\n",
      "Prior Probability of label: 7  =  0.1085\n",
      "Prior Probability of label: 8  =  0.09183333333333334\n",
      "Prior Probability of label: 9  =  0.10016666666666667\n"
     ]
    }
   ],
   "source": [
    "pi_j = []\n",
    "for i in np.unique(train_label):\n",
    "    pi_j.append(sum(train_label['label']==i)/len(train_label))\n",
    "    print (\"Prior Probability of label:\",i,\" = \",pi_j[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2: For each pixel Xi and label j, compute Pji = P(Xi = 1|y = j) (Use the maximum likelihood estimate shown in class). Use Laplacian Smoothing for computing Pji. Report the highest Pji for each label j. (15 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0  Highest Pji: 0.8518518518518519\n",
      "Label 1  Highest Pji: 0.9851411589895989\n",
      "Label 2  Highest Pji: 0.7289879931389366\n",
      "Label 3  Highest Pji: 0.8081967213114755\n",
      "Label 4  Highest Pji: 0.8496\n",
      "Label 5  Highest Pji: 0.7112403100775194\n",
      "Label 6  Highest Pji: 0.8491803278688524\n",
      "Label 7  Highest Pji: 0.7947932618683001\n",
      "Label 8  Highest Pji: 0.8752260397830018\n",
      "Label 9  Highest Pji: 0.867330016583748\n"
     ]
    }
   ],
   "source": [
    "pji = []\n",
    "for j in range(10):\n",
    "    data = train_data[train_label['label']==j]\n",
    "    p = []\n",
    "    for i in range(784):\n",
    "        p.append((sum(data[i]==1)+1)/(len(data)+2))\n",
    "    pji.append(p)\n",
    "\n",
    "for j in range(10):\n",
    "    print (\"Label\",j,\" Highest Pji:\",max(pji[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3: Use naive bayes (as shown in lecture slides) to classify the test data. Report the accuracy. (5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb = [[np.log(pi_j[j]) + sum(test_data.iloc[i]*np.log(pji[j]) + (1-test_data.iloc[i])*np.log(1-np.array(pji[j])))\n",
    "         for j in range(10)]\n",
    "            for i in range(len(test_data))]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_pred = []\n",
    "for i in range(len(nb)):\n",
    "    test_pred.append(nb[i].index(max(nb[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy 0.809\n"
     ]
    }
   ],
   "source": [
    "print (\"Test Accuracy\",((pd.DataFrame(test_pred)==test_label)[0].sum())/len(test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4: Compute the confusion matrix (as shown in the lectures) and report the top 3 pairs with most (absolute number) incorrect classifications. (10 marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 74   0   0   0   0   5   2   0   4   0]\n",
      " [  0 120   0   0   0   4   1   0   1   0]\n",
      " [  1   7  88   4   0   1   2   3   8   2]\n",
      " [  0   2   1  86   1   6   3   2   3   3]\n",
      " [  1   1   1   0  83   0   2   0   1  21]\n",
      " [  3   1   1  11   2  62   2   3   1   1]\n",
      " [  3   0   4   0   3   4  73   0   0   0]\n",
      " [  0   6   2   0   3   1   0  77   3   7]\n",
      " [  0   2   2   9   4   3   1   2  61   5]\n",
      " [  0   1   0   1   4   0   0   0   3  85]]\n"
     ]
    }
   ],
   "source": [
    "cnf_matrix = confusion_matrix(np.array(test_label[0]), np.array(test_pred))\n",
    "print (\"Confusion Matrix:\")\n",
    "print (cnf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum = {}\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if i!=j:\n",
    "            #maximum[(i,j)] = confusion_matrix[i][j]\n",
    "            maximum[cnf_matrix[i][j]] = (i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st Most incorrect classification pair: (4, 9) value: 21\n",
      "2nd Most incorrect classification pair: (5, 3) value: 11\n",
      "3rd Most incorrect classification pair: (8, 3) value: 9\n"
     ]
    }
   ],
   "source": [
    "print (\"1st Most incorrect classification pair:\",maximum[sorted(maximum)[-1]],\"value:\",sorted(maximum)[-1])\n",
    "print (\"2nd Most incorrect classification pair:\",maximum[sorted(maximum)[-2]],\"value:\",sorted(maximum)[-2])\n",
    "print (\"3rd Most incorrect classification pair:\",maximum[sorted(maximum)[-3]],\"value:\",sorted(maximum)[-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5: Visualizing mistakes: Print two MNIST images from the test data that your classifier misclassified. Write both the true and predicted labels for both of these misclassified digits. (10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a379240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Label 5\n",
      "Predicted Label 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a3bdb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Label 5\n",
      "Predicted Label 3\n"
     ]
    }
   ],
   "source": [
    "plt.gray()\n",
    "test_label_list = list(test_label[0].values)\n",
    "count = 0\n",
    "for i in range(len(test_pred)):\n",
    "    if test_label_list[i]!=test_pred[i]:\n",
    "        count = count + 1\n",
    "        plt.imshow(np.reshape(test_data.iloc[i].values, (28,28)))\n",
    "        plt.show()\n",
    "        print (\"True Label\",test_label_list[i])\n",
    "        print (\"Predicted Label\",test_pred[i])\n",
    "        if count==2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will implement Gaussian Mixture Model and Linear Discriminant Analysis on the breast cancer data (sklearn.datasets.load breast cancer) available in sklean.datasets. Load the data and split it into train-validation-test (40-20-40 split). Don’t shuffle the data, otherwise your results will be different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Question 6: Implement Gaussian Mixture model on the data as shown in class. Tune the covariance type parameter on the validation data. Use the selected value to compute the test accuracy. As always, train the model on train+validation data to compute the test accuracy. (10 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "brest_cancer_data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_data_train_val, bc_data_test, bc_label_train_val, bc_label_test = train_test_split(brest_cancer_data.data, brest_cancer_data.target, test_size = 0.4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_data_train, bc_data_val, bc_label_train, bc_label_val = train_test_split(bc_data_train_val, bc_label_train_val, test_size=0.33, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for covariance type full = 0.911504424778761\n",
      "Validation accuracy for covariance type tied = 0.8584070796460177\n",
      "Validation accuracy for covariance type diag = 0.9469026548672567\n",
      "Validation accuracy for covariance type spherical = 0.9734513274336283\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "for cov in ['full', 'tied', 'diag', 'spherical']:\n",
    "    clf = GaussianMixture(n_components=2, covariance_type=cov, random_state=0)  \n",
    "    clf.means_init = np.array([bc_data_train[bc_label_train == i].mean(axis=0) for i in range(2)])\n",
    "    clf.fit(bc_data_train)\n",
    "\n",
    "    pred_val = clf.predict(bc_data_val)\n",
    "    #print (clf.covariances_.shape)\n",
    "    print ('Validation accuracy for covariance type '+ cov + ' = ' + str(accuracy_score(bc_label_val, pred_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above results, covariance_type = 'spherical' is choosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 0.9385964912280702\n"
     ]
    }
   ],
   "source": [
    "# Best for spherical\n",
    "\n",
    "clf = GaussianMixture(n_components=2, covariance_type='spherical', random_state=0) \n",
    "clf.means_init = np.array([bc_data_train_val[bc_label_train_val == i].mean(axis=0) for i in range(2)])\n",
    "\n",
    "clf.fit(bc_data_train_val)\n",
    "pred_test = clf.predict(bc_data_test)\n",
    "print ('Test accuracy = ' + str(accuracy_score(bc_label_test, pred_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 7: Apply Linear Discriminant Analysis model on the train+validation data and report the accuracy obtained on test data. Report the transformation matrix (w) along with the intercept. (5 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "brest_cancer_data = load_breast_cancer()\n",
    "bc_data_train_val, bc_data_test, bc_label_train_val, bc_label_test = train_test_split(brest_cancer_data.data, brest_cancer_data.target, test_size = 0.4, shuffle=False)\n",
    "\n",
    "# Intialize\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "# Train\n",
    "clf.fit(bc_data_train_val, bc_label_train_val)\n",
    "# Test\n",
    "pred_test = clf.predict(bc_data_test)\n",
    "\n",
    "# print the accuracy\n",
    "print ('Test accuracy = ' + str(np.sum(pred_test == bc_label_test)/len(bc_label_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation matrix:\n",
      " [[ 1.44850540e+00]\n",
      " [-5.78558342e-02]\n",
      " [-1.63287905e-01]\n",
      " [-2.41591226e-03]\n",
      " [-1.47998227e+01]\n",
      " [ 2.02002879e+01]\n",
      " [-1.35274499e+00]\n",
      " [-9.28332478e+00]\n",
      " [ 4.70449033e+00]\n",
      " [-1.20819489e+01]\n",
      " [-2.28593080e+00]\n",
      " [-6.86881771e-02]\n",
      " [ 1.57174631e-01]\n",
      " [ 2.87696014e-03]\n",
      " [-7.62195534e+01]\n",
      " [-7.13914326e-01]\n",
      " [ 1.15732894e+01]\n",
      " [-4.54646862e+01]\n",
      " [ 3.43973413e+00]\n",
      " [ 5.46385722e+01]\n",
      " [-9.62759711e-01]\n",
      " [-1.96127587e-02]\n",
      " [-1.50437998e-02]\n",
      " [ 6.50322727e-03]\n",
      " [ 3.77295366e+00]\n",
      " [ 2.82494809e-01]\n",
      " [-1.64540854e+00]\n",
      " [-4.01125170e+00]\n",
      " [-4.42138568e+00]\n",
      " [-2.18611650e+01]]\n",
      "\n",
      "Intercept:\n",
      " [50.95842876]\n"
     ]
    }
   ],
   "source": [
    "print (\"Transformation matrix:\\n\",clf.scalings_)\n",
    "print (\"\\nIntercept:\\n\",clf.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Evaluating Classifiers\n",
    "Question 8: Load the digits dataset (scikit-learn’s toy dataset) and take the last 1300 samples as your test set. Train a K-Nearest Neighbor (k=5, linf distance) model and then without using any scikit-learn method, report the final values for Specificity, Sensitivity, TPR, TNR, FNR, FPR, Precision and Recall for Digit 3 (this digit is a positive, everything else is a negative). (15 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_data = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_train = digits_data['data'][0:len(digits_data['data'])-1300]\n",
    "digits_train_label = digits_data['target'][0:len(digits_data['target'])-1300]\n",
    "digits_test = digits_data['data'][len(digits_data['data'])-1300:]\n",
    "digits_test_label = digits_data['target'][len(digits_data['target'])-1300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.8938461538461538\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(5, p=np.inf)\n",
    "clf.fit(digits_train, digits_train_label)\n",
    "\n",
    "pred_test = clf.predict(digits_test)\n",
    "print ('accuracy = ' + str(np.sum(pred_test == digits_test_label)/(len(digits_test_label))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = np.zeros((10,10))\n",
    "\n",
    "for i in range(len(pred_test)):\n",
    "    confusion_matrix[digits_test_label[i]][pred_test[i]] = confusion_matrix[digits_test_label[i]][pred_test[i]] + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\n",
      " [[126   0   0   0   1   0   0   0   0   0]\n",
      " [  1  90  23   1   0   0   3   0  11   1]\n",
      " [  2   0 120   4   0   0   0   0   1   1]\n",
      " [  0   1   4 113   0   1   0   2   2   7]\n",
      " [  3   1   0   0 118   1   5   1   3   1]\n",
      " [  1   0   0   5   0 121   4   0   0   1]\n",
      " [  0   0   0   0   0   0 129   0   1   0]\n",
      " [  0   0   0   0   0   1   0 128   0   1]\n",
      " [  1   6   8   0   0   2   1   1 107   2]\n",
      " [  1   9   0   3   0   4   0   2   3 110]]\n"
     ]
    }
   ],
   "source": [
    "print (\"Confusion Matrix:\\n\\n\", confusion_matrix.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_digit_3 = np.matrix([[confusion_matrix[3][3], sum(confusion_matrix[3])-confusion_matrix[3][3]], \\\n",
    "               [sum(confusion_matrix[:,3])-confusion_matrix[3][3], confusion_matrix.sum()-sum(confusion_matrix[3])-sum(confusion_matrix[:,3])+confusion_matrix[3][3]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pred_3</th>\n",
       "      <th>Pred_not_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual_3</th>\n",
       "      <td>113</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual_not_3</th>\n",
       "      <td>13</td>\n",
       "      <td>1157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Pred_3  Pred_not_3\n",
       "Actual_3         113          17\n",
       "Actual_not_3      13        1157"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_confusion_matrix_digit_3 = pd.DataFrame(confusion_matrix_digit_3, columns=['Pred_3','Pred_not_3'], index=['Actual_3','Actual_not_3'], dtype=int)\n",
    "df_confusion_matrix_digit_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>113</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>13</td>\n",
       "      <td>1157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Positive  Negative\n",
       "True        113        17\n",
       "False        13      1157"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_confusion_matrix_digit_3 = pd.DataFrame(confusion_matrix_digit_3, columns=['Positive','Negative'], index=['True','False'], dtype=int)\n",
    "df_confusion_matrix_digit_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Specificity = confusion_matrix_digit_3[1,1]/confusion_matrix_digit_3[1].sum()\n",
    "Sensitivity = confusion_matrix_digit_3[0,0]/confusion_matrix_digit_3[0].sum()\n",
    "Precision = confusion_matrix_digit_3[0,0]/confusion_matrix_digit_3[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "TPR = confusion_matrix_digit_3[0,0]/confusion_matrix_digit_3[0].sum()\n",
    "TNR = confusion_matrix_digit_3[1,1]/confusion_matrix_digit_3[1].sum()\n",
    "FNR = confusion_matrix_digit_3[0,1]/confusion_matrix_digit_3[0].sum()\n",
    "#FNR=1-TPR\n",
    "FPR = confusion_matrix_digit_3[1,0]/confusion_matrix_digit_3[1].sum()\n",
    "Recall = TPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity: 0.9888888888888889\n",
      "Sensitivity: 0.8692307692307693\n",
      "Precision: 0.8968253968253969\n",
      "TPR: 0.8692307692307693\n",
      "TNR: 0.9888888888888889\n",
      "FNR: 0.13076923076923078\n",
      "FPR: 0.011111111111111112\n",
      "Recall: 0.8692307692307693\n"
     ]
    }
   ],
   "source": [
    "print (\"Specificity:\",Specificity)\n",
    "print (\"Sensitivity:\",Sensitivity)\n",
    "print (\"Precision:\",Precision)\n",
    "print (\"TPR:\",TPR)\n",
    "print (\"TNR:\",TNR)\n",
    "print (\"FNR:\",FNR)\n",
    "print (\"FPR:\",FPR)\n",
    "print (\"Recall:\",Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 5 Regression\n",
    "An ablation experiment consists of removing one feature from an experiment, in order to assess the amount of additional information that feature provides above and beyond the others. For this section, we will use the diabetes dataset from scikit-learn’s toy datasets. Split the data into training and testing data as a 90-10 split with random state of 10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_data = load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_data_train, diabetes_data_test, diabetes_data_label_train, diabetes_data_label_test = train_test_split(diabetes_data.data, diabetes_data.target, test_size = 0.1, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 9: Perform least squares regression on this dataset. Report the mean squared error and the mean absolute error on the test data. (5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Least squares regression\n",
    "theta,residuals,rank,s = np.linalg.lstsq(diabetes_data_train, diabetes_data_label_train, rcond=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28060.62255931054\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = np.dot(diabetes_data_test, theta)\n",
    "# Let's see the output on training data as well, to see the training error\n",
    "y_true_pred = np.dot(diabetes_data_train, theta)\n",
    "# MSE calculation\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print (mean_squared_error(diabetes_data_label_test, predictions))\n",
    "#print (mean_squared_error(diabetes_data_label_train, y_true_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160.8439534334583\n"
     ]
    }
   ],
   "source": [
    "# MAE calculation\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print (mean_absolute_error(diabetes_data_label_test, predictions))\n",
    "#print (mean_absolute_error(diabetes_data_label_train, y_true_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 10: Repeat the experiment from Question 10 for all possible values of ablation (i.e., removing the feature 1 only, then removing the feature 2 only, and so on). Report all MSEs. (10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature removed: 0   MSE: 25835.45614572961\n",
      "Feature removed: 1   MSE: 25963.509410892115\n",
      "Feature removed: 2   MSE: 26117.78113034645\n",
      "Feature removed: 3   MSE: 25932.067639245513\n",
      "Feature removed: 4   MSE: 25837.2062805149\n",
      "Feature removed: 5   MSE: 25837.11626632769\n",
      "Feature removed: 6   MSE: 25849.17168725327\n",
      "Feature removed: 7   MSE: 25835.828130257145\n",
      "Feature removed: 8   MSE: 25920.1352209274\n",
      "Feature removed: 9   MSE: 25846.88235000444\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    data = pd.DataFrame(diabetes_data_train)\n",
    "    del data[i]\n",
    "    data = np.array(data)\n",
    "    theta,residuals,rank,s = np.linalg.lstsq(data, diabetes_data_label_train, rcond=None)\n",
    "    diabetes_train_pred = np.dot(data, theta)\n",
    "    print (\"Feature removed:\",i,\"  MSE:\",mean_squared_error(diabetes_data_label_train, diabetes_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 11: Based on the MSE values obtained from Question 11, which features do you deem the most/least significant and why? (5 marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Removed [0, 4, 5, 7, 9] MSE: 25851.38584730351\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame(diabetes_data_train)\n",
    "#data_test = pd.DataFrame(diabetes_data_test)\n",
    "del data[0], data[4], data[5], data[7], data[9]\n",
    "#del data_test[0], data_test[7], data_test[4], data_test[5], data_test[9]\n",
    "data = np.array(data)\n",
    "#data_test = np.array(data_test)\n",
    "theta,residuals,rank,s = np.linalg.lstsq(data, diabetes_data_label_train, rcond=None)\n",
    "diabetes_train_pred = np.dot(data, theta)\n",
    "print (\"Features Removed [0, 4, 5, 7, 9] MSE:\",mean_squared_error(diabetes_data_label_train, diabetes_train_pred))\n",
    "#diabetes_test_pred = np.dot(data_test, theta)\n",
    "#print (\"Test MSE:\",mean_squared_error(diabetes_data_label_test, diabetes_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Removed [1, 2, 3, 6, 8] MSE: 27048.113570844143\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame(diabetes_data_train)\n",
    "#data_test = pd.DataFrame(diabetes_data_test)\n",
    "del data[1], data[2], data[3], data[6], data[8]\n",
    "#del data_test[0], data_test[7], data_test[4], data_test[5], data_test[9]\n",
    "data = np.array(data)\n",
    "#data_test = np.array(data_test)\n",
    "theta,residuals,rank,s = np.linalg.lstsq(data, diabetes_data_label_train, rcond=None)\n",
    "diabetes_train_pred = np.dot(data, theta)\n",
    "print (\"Features Removed [1, 2, 3, 6, 8] MSE:\",mean_squared_error(diabetes_data_label_train, diabetes_train_pred))\n",
    "#diabetes_test_pred = np.dot(data_test, theta)\n",
    "#print (\"Test MSE:\",mean_squared_error(diabetes_data_label_test, diabetes_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the Above Analysis for the Most significant features MSE increased a lot and for Least significant features MSE didnt increase much\n",
      "Most Significant feature : [1, 2, 3, 6, 8]\n",
      "Least Significant feature: [0, 4, 5, 7, 9]\n"
     ]
    }
   ],
   "source": [
    "print (\"From the Above Analysis for the Most significant features MSE increased a lot and for Least significant features MSE didnt increase much\")\n",
    "print (\"Most Significant feature : [1, 2, 3, 6, 8]\")\n",
    "print (\"Least Significant feature: [0, 4, 5, 7, 9]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
